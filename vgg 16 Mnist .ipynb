{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-1b6ff1cc1ee8>:1: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From C:\\Users\\Bhavana\\Anaconda3.1\\envs\\tensorflow-cpu-keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Bhavana\\Anaconda3.1\\envs\\tensorflow-cpu-keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Bhavana\\Anaconda3.1\\envs\\tensorflow-cpu-keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Bhavana\\Anaconda3.1\\envs\\tensorflow-cpu-keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Bhavana\\Anaconda3.1\\envs\\tensorflow-cpu-keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Bhavana\\Anaconda3.1\\envs\\tensorflow-cpu-keras\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images  # Returns np.array\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images  # Returns np.array\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.shape\n",
    "index = 7\n",
    "plt.imshow(train_data[index].reshape(28, 28))\n",
    "print (\"y = \" + str(np.squeeze(train_labels[index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 55000\n",
      "number of evaluation examples = 10000\n",
      "X_train shape: (55000, 784)\n",
      "Y_train shape: (55000,)\n",
      "X_test shape: (10000, 784)\n",
      "Y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print (\"number of training examples = \" + str(train_data.shape[0]))\n",
    "print (\"number of evaluation examples = \" + str(eval_data.shape[0]))\n",
    "print (\"X_train shape: \" + str(train_data.shape))\n",
    "print (\"Y_train shape: \" + str(train_labels.shape))\n",
    "print (\"X_test shape: \" + str(eval_data.shape))\n",
    "print (\"Y_test shape: \" + str(eval_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    # Input Layer\n",
    "    input_height, input_width = 28, 28\n",
    "    input_channels = 1\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, input_height, input_width, input_channels])\n",
    "\n",
    "    # Convolutional Layer #1 and Pooling Layer #1\n",
    "    conv1_1 = tf.layers.conv2d(inputs=input_layer, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv1_2 = tf.layers.conv2d(inputs=conv1_1, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
    "    \n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2_1 = tf.layers.conv2d(inputs=pool1, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv2_2 = tf.layers.conv2d(inputs=conv2_1, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
    "\n",
    "    # Convolutional Layer #3 and Pooling Layer #3\n",
    "    conv3_1 = tf.layers.conv2d(inputs=pool2, filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv3_2 = tf.layers.conv2d(inputs=conv3_1, filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
    "\n",
    "    # Convolutional Layer #4 and Pooling Layer #4\n",
    "    conv4_1 = tf.layers.conv2d(inputs=pool3, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv4_2 = tf.layers.conv2d(inputs=conv4_1, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool4 = tf.layers.max_pooling2d(inputs=conv4_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
    "\n",
    "    # Convolutional Layer #5 and Pooling Layer #5\n",
    "    conv5_1 = tf.layers.conv2d(inputs=pool4, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv5_2 = tf.layers.conv2d(inputs=conv5_1, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    pool5 = tf.layers.max_pooling2d(inputs=conv5_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
    "\n",
    "    # FC Layers\n",
    "    pool5_flat = tf.contrib.layers.flatten(pool5)\n",
    "    FC1 = tf.layers.dense(inputs=pool5_flat, units=4096, activation=tf.nn.relu)\n",
    "    FC2 = tf.layers.dense(inputs=FC1, units=4096, activation=tf.nn.relu)\n",
    "    FC3 = tf.layers.dense(inputs=FC2, units=1000, activation=tf.nn.relu)\n",
    "\n",
    "    \n",
    "    dropout = tf.layers.dropout(inputs=FC3, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # Logits Layer or the output layer. return the raw values for our predictions.\n",
    "    # Like FC layer, logits layer is another dense layer. \n",
    "    # apply the softmax\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "    \n",
    "   \n",
    "    predictions = {\n",
    "       \n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    \n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "   \n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(labels=labels,\n",
    "                                        predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": train_data},\n",
    "                                                    y=train_labels,\n",
    "                                                    batch_size=200,\n",
    "                                                    num_epochs=1,\n",
    "                                                    shuffle=False)\n",
    "mnist_classifier.train(input_fn=train_input_fn,\n",
    "                       steps=None,\n",
    "                       hooks=None)\n",
    "\n",
    "\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": eval_data},\n",
    "                                                   y=eval_labels,\n",
    "                                                   num_epochs=1,\n",
    "                                                   shuffle=False)\n",
    "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
